{"name":"Pair-trading","tagline":"CS7641 project","body":"## Introduction to Pairs Trading\r\n\r\nThe primary goal in an investment endeavor is the implementation of strategies that minimize the risk while also maximizing the financial gain or return from the said investment. While there have been many popular strategies and techniques developed over the years that point towards the same goal, the 'Pairs-Trading' strategy is one that has been used to great extent in modern hedge-funds, for its simplicity and inherent market-neutral qualities. This strategy, often termed a statistical-arbitrage, relies on monitoring the correlation between a pair of stocks (known to be correlated). A long position is opened on the stock that rises and a short position is opened on the stock that falls. The underlying assumption in pairs-trading is that pairs of stocks, that have historically shown similarities in their behavior will eventually converge in the long run, even if they diverge in the short term, allowing the trader to profit off the pair regardless of the market. \r\n\r\nIn such a strategy, identification of correlated stocks and generation of pairs is of paramount importance. In this project, we employ unsupervised learning techniques that include Density-Based Spatial Cluster of Applications with Noise and K-Means Algorithm. Once, the relevant pairs have been identified, their price relations are extrapolated using supervised learning techniques such as Linear Regression. This overall methodology will help provide insight into the relations between various stocks and facilitate the generation of appropriate trading strategies for them.  \r\n\r\n## Dataset and Preprocessing\r\n\r\nThe datasets are provided by Wharton Research Data Services (WRDS). We mainly obtained the daily stock files from file from CRSP and quarterly fundamentals from Compustats for our purpose. Initially, our dataset consists of stock price files from 3000 stocks which are constituents of Russell 3000. Those stocks' value and size are large enough to restore the whole market value, representing approximately 95% of the total market shares. We performed this pre-screening process to avoid the 'small-cap' trap in the market. Currently, there are more than 6000 active stocks in the U.S. Stock Market but most of them are micro-valued. In reality, investors often cautiously avoid investing in those stocks, since trading, even a small number of shares might have unpredictable effects on their stock prices. We should keep this in mind when doing academic research. We set the sample period from 2010-01-01 to 2015-12-31 for training strategies and use sample period 2016-01-01 to 2019-12-31 for backtesting. \r\n\r\nIn our next stage, we want to pre-select eligible stocks that enable us to sail through further steps. First, we removed stocks that were delisted, exchanged, or merged during our sample period since those stocks are no longer tradable. Next, we removed stocks that have negative prices which will be problematic for further analysis. Stocks that are constantly trading at-low-volume also have to be removed since improper trading executions can largely change their stock prices and altered history. Finally, we remove stocks that have more than half missing prices. A similar approach was performed on the financial fundamentals of datasets. In the end, there are 1795 eligible stocks for further analysis. \r\n\r\n## Principal Component Analysis and Clustering Analysis\r\n\r\n### Principal Component Analysis\r\nConsidering the very high number of features within the dataset (which inlcudes both realtime stock data as well as several financial ratios), it is pertienent to use Principal Component Analysis to reduce the dimensionality of the dataset to ease computation. For this methodology, PCA must be performed independantly on the time series stock price data and the financial ratios. It should also be noted that each datapoint in the time searies data is considered to be 1 individal feature. After PCA, the time series data is reduced to 50 principal components and the financial ratios are reduced to 5 principal components. The resultant reduced datasets are then concatenated to create a 55 dimensional training dataset which is then used for clustering.\r\n\r\nTwo clustering algorithms were explored to create clusters of stocks: \r\n### Density-based spatial clustering of applications with Noise\r\nThe DBSCAN algorithm was paramterized by eps = 1.8 and minPoints = 3 which resulted in the formation of 11 clusters. A simple visualization of the cluster in the form of a T-SNE plot is shown below:\r\n![T-SNE plot for DBSCAN](https://raw.githubusercontent.com/daehkim/pair-trading/master/pictures/DBSCAN_plots/T-SNE_plot_for_stock_clusters.png)\r\nThe following figure shows the number of members in each cluster, demontrating the fact that a huge proportion of the stocks are bunched into a single cluster. This disproportionate distribution of the stocks in clusters is expected to some extent, since the dataset is possibly dominated by stocks from a single or closely related industries.\r\n![Cluster Member counts for DBSCAN](https://raw.githubusercontent.com/daehkim/pair-trading/master/pictures/DBSCAN_plots/cluster_member_counts.png)\r\n\r\nIn order to increase confidence in the clustering procedure, the real time series stock price data of the stocks in each cluster were also investigated. The time series data of the stocks in 4 of the 11 clusters are illustrated below. From a visual perspective, stocks within the same cluster do show a realtively high correlation among them in terms of the behavior of the stock prices. \r\n![Stock price in each cluster](https://raw.githubusercontent.com/daehkim/pair-trading/master/pictures/DBSCAN_plots/combined_time_Series.png)\r\n\r\n### KMeans Clustering\r\nThe KMeans clustering algorithm is a popular clustering methodolgy employed in pair-trading implementeations. The most important aspect of this algorithm is the determination of the number of clusters. This can be ascertained using an elbow-method based cross-validation technique. There are three loss-metrics (or scores) that can be used in the elbow method which are: \r\n\r\n1) Distortion Score:\r\n2) Silhouette Score:\r\n3) Calinski Harabz Score:\r\n\r\nThe elbow for each of the above mentioned score is illustrated below. An average of the elbow from each of these independant metrics was finally used in training the KMeans Algorithm. \r\n![Elbow Plots for KMEANS](https://raw.githubusercontent.com/daehkim/pair-trading/master/pictures/Kmeans_plots/elbow.PNG)\r\n\r\nThe following plot shows a visualization of the clustered datapoints in the form of a T-SNE plot. Again, similar to what was observed with DBSCAN, we notice a slight disproportionality in the size of each cluster, which as mentioned before, can be expected. \r\n![Cluster Member counts for DBSCAN](https://raw.githubusercontent.com/daehkim/pair-trading/master/pictures/Kmeans_plots/T_SNE_kmeans.png)\r\n### Pair selection\r\n\r\n## Trading Strategy\r\n\r\nIn this section, we will discuss how we generate the z-score history by stock pair's price history. We generate the z-score history to decide when we long and short the stocks. The z-score is simply (spread)/(standard deviation of spread) and spread is calculated based on the stock pair's price history. The basic method to calculate the spread is using a log of prices of stocks A and B.\r\nSpread = log(a) - nlog(b), where 'a' and 'b' are prices of stocks A and B respectively. The 'n' is the hedge ratio which is constant.\r\nCalculate 'n' using regression so that spread is as close to 0 as possible. Also, since stocks A and B are cointegrated, the spread tends to converge to 0. To calculate the spread, we used the polynomial linear regression and linear regression with the Kalman filter. The data used to calculate the spread is the history of the stocks' prices for the previous 700 days. \r\n\r\n### Lnear Regression\r\n\r\nWe used the log of stock A's prices as data points and the log of stock B's prices as a label. We train the model with these datasets. After we generate the model, we predict the log(b) and calculate the spread as:\r\n\r\nSpread = lr.pred(log(a)) - log(b)\r\n\r\nIt also leads us to calculate the z-score by the following equation:\r\n\r\nz-score = Spread / standard deviation\r\n\r\nThe standard deviation is calculated by training data, which is the training data prices' spread history.\r\nWe also used the degree = 4 for the polynomial linear regression hyperparameter. If it becomes too big, it goes to overfitting and will not generate the spread. If the spread distribution is small, it is hard to decide when we long and short the stocks. Here is the example graph of z-score history for the stock pairs we have. You can see it converges.\r\n\r\n![z-score](https://raw.githubusercontent.com/daehkim/pair-trading/master/pictures/each_pair_z_score.png)\r\n\r\n### Linear Regression with Kalman Filter\r\n(Zhenyu Jia)\r\n\r\n## Backtesting\r\n\r\nIn this section, we will discuss testing. We apply our trading strategy to the real stock market and check how much we can earn based on our approach. We used the moving windows approach for the testing. For the training data, we used the previous 700 days stock prices. After we train the model with our machine learning algorithm, we calculate the z-score with the generated model and decide whether we will long or short the stocks. The input of backtesting is the z-score history generated in the 'trading strategy' part and the price history. Based on the input, we keep calculating the earning and loss of our stock and inverse. We also track the total asset history and return it as an output of backtesting.\r\n\r\n### Implementation\r\n\r\nTo simplify the backtesting, we just set the initial money as million dollars and the volume of the stocks we trading as 'total assets' / '# of pairs'. Therefore, if our current total asset is $100 and the number of stock pairs is 10, we long/short the stock only with $10. We also calculate the price of the inverse (short) in the everyday base and we didn't consider the commission of trading to simplify.\r\n\r\n### Results\r\n\r\nWe run the backtesting for all the timeline (2007~2015). Here are all the results from the backtesting. The x-label is the daily based time. It does not include market off-day. The y-label is the money (dollars).\r\n\r\n#### Each pair's assets\r\n\r\n![each assets](https://raw.githubusercontent.com/daehkim/pair-trading/master/pictures/each_pair_assets.png)\r\n\r\n#### Total assets\r\n\r\n![total assets](https://raw.githubusercontent.com/daehkim/pair-trading/master/pictures/total_assets.png)\r\n\r\n\r\n```js\r\n// Javascript code with syntax highlighting.\r\nvar fun = function lang(l) {\r\n  dateformat.i18n = require('./lang/' + l)\r\n  return true;\r\n}\r\n```\r\n\r\n```ruby\r\n# Ruby code with syntax highlighting\r\nGitHubPages::Dependencies.gems.each do |gem, version|\r\n  s.add_dependency(gem, \"= #{version}\")\r\nend\r\n```\r\n\r\n#### Header 4\r\n\r\n*   This is an unordered list following a header.\r\n*   This is an unordered list following a header.\r\n*   This is an unordered list following a header.\r\n\r\n##### Header 5\r\n\r\n1.  This is an ordered list following a header.\r\n2.  This is an ordered list following a header.\r\n3.  This is an ordered list following a header.\r\n\r\n###### Header 6\r\n\r\n| head1        | head two          | three |\r\n|:-------------|:------------------|:------|\r\n| ok           | good swedish fish | nice  |\r\n| out of stock | good and plenty   | nice  |\r\n| ok           | good `oreos`      | hmm   |\r\n| ok           | good `zoute` drop | yumm  |\r\n\r\n### There's a horizontal rule below this.\r\n\r\n* * *\r\n\r\n### Here is an unordered list:\r\n\r\n*   Item foo\r\n*   Item bar\r\n*   Item baz\r\n*   Item zip\r\n\r\n### And an ordered list:\r\n\r\n1.  Item one\r\n1.  Item two\r\n1.  Item three\r\n1.  Item four\r\n\r\n### And a nested list:\r\n\r\n- level 1 item\r\n  - level 2 item\r\n  - level 2 item\r\n    - level 3 item\r\n    - level 3 item\r\n- level 1 item\r\n  - level 2 item\r\n  - level 2 item\r\n  - level 2 item\r\n- level 1 item\r\n  - level 2 item\r\n  - level 2 item\r\n- level 1 item\r\n\r\n### Small image\r\n\r\n![Octocat](https://github.githubassets.com/images/icons/emoji/octocat.png)\r\n\r\n### Large image\r\n\r\n![Branching](https://guides.github.com/activities/hello-world/branching.png)\r\n\r\n\r\n### Definition lists can be used with HTML syntax.\r\n\r\n<dl>\r\n<dt>Name</dt>\r\n<dd>Godzilla</dd>\r\n<dt>Born</dt>\r\n<dd>1952</dd>\r\n<dt>Birthplace</dt>\r\n<dd>Japan</dd>\r\n<dt>Color</dt>\r\n<dd>Green</dd>\r\n</dl>\r\n\r\n```\r\nLong, single-line code blocks should not wrap. They should horizontally scroll if they are too long. This line should be long enough to demonstrate this.\r\n```\r\n\r\n\r\n```\r\nThe final element.\r\n```\r\n\r\n## Contribution\r\n- Daehyun Kim\r\n  - Trading Strategy Structure\r\n  - Trading Strategy Algorithm (Linear Regression)\r\n  - Backtesting\r\n\r\n## Reference\r\nhttps://blog.quantinsti.com/pairs-trading-basics/\r\n\r\nhttps://en.wikipedia.org/wiki/Pairs_trade\r\n","note":"Don't delete this file! It's used internally to help with page regeneration."}